{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stories for the challenge: two_supporting_facts_10k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekremguzelyel/anaconda/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Vocab size: 36 unique words\n",
      "Story max length: 552 words\n",
      "Query max length: 5 words\n",
      "Number of training stories: 10000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'got', 'the', 'football', 'there', '.', 'John', 'went', 'to', 'the', 'kitchen', '.', 'Mary', 'went', 'back', 'to', 'the', 'kitchen', '.', 'Mary', 'went', 'back', 'to', 'the', 'garden', '.'], ['Where', 'is', 'the', 'football', '?'], 'garden')\n",
      "-\n",
      "Vectorizing the word sequences...\n",
      "-\n",
      "inputs: integer tensor of shape (samples, max_length)\n",
      "inputs_train shape: (10000, 552)\n",
      "inputs_test shape: (1000, 552)\n",
      "-\n",
      "queries: integer tensor of shape (samples, max_length)\n",
      "queries_train shape: (10000, 5)\n",
      "queries_test shape: (1000, 5)\n",
      "-\n",
      "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
      "answers_train shape: (10000,)\n",
      "answers_test shape: (1000,)\n",
      "-\n",
      "Compiling...\n",
      "0\n",
      "input_sequence (?, 552)\n",
      "question (?, 5)\n",
      "input_encoded_m (?, 552, 64)\n",
      "input_encoded_c (?, 552, 64)\n",
      "question_encoded (?, 5, 64)\n",
      "match (?, 552, 5)\n",
      "match (?, 552, 5)\n",
      "response (?, 5, 64)\n",
      "response (?, 5, 64)\n",
      "1\n",
      "input_sequence (?, 552)\n",
      "question (?, 5)\n",
      "input_encoded_m (?, 552, 64)\n",
      "input_encoded_c (?, 552, 64)\n",
      "question_encoded (?, 5, 64)\n",
      "match (?, 552, 5)\n",
      "match (?, 552, 5)\n",
      "response (?, 5, 64)\n",
      "response (?, 5, 64)\n",
      "2\n",
      "input_sequence (?, 552)\n",
      "question (?, 5)\n",
      "input_encoded_m (?, 552, 64)\n",
      "input_encoded_c (?, 552, 64)\n",
      "question_encoded (?, 5, 64)\n",
      "match (?, 552, 5)\n",
      "match (?, 552, 5)\n",
      "response (?, 5, 64)\n",
      "response (?, 5, 64)\n",
      "(?, 5, 64)\n",
      "(?, 32)\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 30s 3ms/step - loss: 2.2052 - acc: 0.1670 - val_loss: 1.8430 - val_acc: 0.1570\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 44s 4ms/step - loss: 1.8586 - acc: 0.1646 - val_loss: 1.8070 - val_acc: 0.1670\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 54s 5ms/step - loss: 1.8134 - acc: 0.1768 - val_loss: 1.7962 - val_acc: 0.1870\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 54s 5ms/step - loss: 1.8258 - acc: 0.1712 - val_loss: 1.7948 - val_acc: 0.1900\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 51s 5ms/step - loss: 1.8092 - acc: 0.1714 - val_loss: 1.7917 - val_acc: 0.1870\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 50s 5ms/step - loss: 1.8002 - acc: 0.1704 - val_loss: 1.7932 - val_acc: 0.1580\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 1.7977 - acc: 0.1645 - val_loss: 1.7924 - val_acc: 0.1570\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 1.7971 - acc: 0.1699 - val_loss: 1.7911 - val_acc: 0.1870\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 50s 5ms/step - loss: 1.7964 - acc: 0.1727 - val_loss: 1.7921 - val_acc: 0.1670\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 53s 5ms/step - loss: 1.7969 - acc: 0.1720 - val_loss: 1.7939 - val_acc: 0.1670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129478c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Train a memory network on the bAbI dataset.\n",
    "References:\n",
    "- https://github.com/keras-team/keras/blob/master/examples/babi_memnn.py\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  [\"Towards AI-Complete Question Answering:\n",
    "  A Set of Prerequisite Toy Tasks\"](http://arxiv.org/abs/1502.05698)\n",
    "- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  [\"End-To-End Memory Networks\"](http://arxiv.org/abs/1503.08895)\n",
    "Reaches 94.3% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n",
    "Time per epoch: 4s on CPU (core i7).\n",
    "\n",
    "The components like tokenizer and parser are taken from the keras implementaion.\n",
    "Since they are specific to the bAbI questions, we haven't changed them.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        answers.append(word_idx[answer])\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
    "          '.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_'\n",
    "                                  'single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_'\n",
    "                                'two-supporting-facts_{}.txt',\n",
    "}\n",
    "challenge_type = 'two_supporting_facts_10k'\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "with tarfile.open(path) as tar:\n",
    "    train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "    test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')\n",
    "\n",
    "# placeholders\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64, \n",
    "                               input_length=query_maxlen))\n",
    "# question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "#                                output_dim=64))\n",
    "\n",
    "question_encoder.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "input_encoded_m = input_encoder_m(inputs=input_sequence) # Meaning that \"inputs = input_sequence\"\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "num_hops=3\n",
    "for i in range(num_hops):\n",
    "    print(i)\n",
    "    \n",
    "    # embed the question into a sequence of vectors\n",
    "    \n",
    "    \n",
    "    # output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "    # encode input sequence and questions (which are indices)\n",
    "    # to sequences of dense vectors\n",
    "    \n",
    "    \n",
    "    #DEBUG\n",
    "    print(\"input_sequence\", input_sequence.get_shape())\n",
    "    print(\"question\", question.get_shape())\n",
    "    print(\"input_encoded_m\", input_encoded_m.get_shape())\n",
    "    print(\"input_encoded_c\", input_encoded_c.get_shape())\n",
    "    print(\"question_encoded\", question_encoded.get_shape())\n",
    "    \n",
    "    \n",
    "    # compute a 'match' between the first input vector sequence\n",
    "    # and the question vector sequence\n",
    "    # shape: `(samples, story_maxlen, query_maxlen)`\n",
    "    match = dot([input_encoded_m, question_encoded], axes=2)\n",
    "    print(\"match\", match.get_shape())\n",
    "    \n",
    "    match = Activation('softmax')(match)\n",
    "    print(\"match\", match.get_shape())\n",
    "    \n",
    "    # add the match matrix with the second input vector sequence\n",
    "    response = dot([match, input_encoded_c], axes=1)  # (samples, story_maxlen, query_maxlen)\n",
    "    print(\"response\", response.get_shape())\n",
    "#     response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "    print(\"response\", response.get_shape())\n",
    "#     print(response.get_shape())\n",
    "#     question = response\n",
    "#     question_decoder # IMPLEMENT THIS\n",
    "#     input_sequence = response\n",
    "#     question_encoded = dot([response, question_encoded], axes=-1)\n",
    "    \n",
    "    \n",
    "    question_encoded = add([response, question_encoded])\n",
    "# #     answer = concatenate([response, question_encoded])\n",
    "    \n",
    "#     print(\"answer\", answer.get_shape())\n",
    "#     if i==0:\n",
    "#         final_ans = answer\n",
    "#     else:\n",
    "#         final_ans = add([final_ans, answer])\n",
    "#     print(\"final\", final_ans.get_shape())\n",
    "print(question_encoded.get_shape())\n",
    "# concatenate the match matrix with the question vector sequence\n",
    "# answer = add([response, question_encoded])\n",
    "\n",
    "# print(\"answer\", answer.get_shape())\n",
    "\n",
    "\n",
    "# the original paper uses a matrix multiplication for this reduction step.\n",
    "# we choose to use a RNN instead.\n",
    "# answer = LSTM(32, return_sequences=True)(answer)  # (samples, 32)\n",
    "\n",
    "# # one regularization layer -- more would probably be needed.\n",
    "# answer = Dropout(0.3)(answer)\n",
    "final_ans = LSTM(32)(question_encoded)\n",
    "final_ans = Dropout(0.3)(final_ans)\n",
    "print(final_ans.get_shape())\n",
    "final_ans = Dense(vocab_size)(final_ans)  # (samples, vocab_size)\n",
    "\n",
    "#     input_sequence = response\n",
    "\n",
    "    \n",
    "# we output a probability distribution over the vocabulary\n",
    "final_ans = Activation('softmax')(final_ans)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], final_ans)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.evaluate(test_stories, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.plot(model.history.history['acc'])\n",
    "plt.plot(model.history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lstm32hops3.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
